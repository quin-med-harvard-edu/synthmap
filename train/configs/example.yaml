# Mode 
mode: "train"

# Training 
name: "test1"
gpu: 0
epochs: 2000
batchsize: 32
lr: 0.0001 # actual learning rate used initially 
#lr_schedule: {'10': 0.000008, '30': 0.0000008, '45': 0.00000008, '60': 0.000000008}
  
lr_type: null
valsize: 32
loss: "mse"                                                                       # ['ivim_loss', 'mse']
selfsupervised: False
userinput: False
                
                

# Input - alternative input 
xdir: '/home/ch215616/w/code/mwf/experiments/mwf_generate_10_000_brains/data/anima_popvar_mwf_20_std2mean_10_v2_CSF_nonlin_10_000brains/signals_anima_normalized/'
ydir: '/home/ch215616/w/code/mwf/experiments/mwf_generate_10_000_brains/data/anima_popvar_mwf_20_std2mean_10_v2_CSF_nonlin_10_000brains/final/mwf_slices/'
xprefix: 'signals_'
yprefix: 'mwf_'
xsuffix: ".nii.gz"
ysuffix: ".nii.gz"
volumes: [50, 386] # will be generated as list 
slices: [150,210]  # will be generated as list 
excludevolumes: [1894,6001] # list of volumes to exclude 
excludevolumesrange: null
seed: 1337


# Input - pass a list of files (through regexp) or a text file (that contains the list of slices)
x: "" #null 
y: "" #null 
xval: "" # null 
yval: "" #null 
xtest: null
ytest: null
shape: [160,192]                                                                        # [[192,192], [192,160]]
input_type: "mwf"                                                                      # ['mwf', 'ivim']
input_nc: 32                                                                             # [32,7]
output_nc: 1                                                                            # [1,4]
visualize_val: null                                                                      # [1,4]
visualize_train: null
visualize_test: null


# Input Modifications 
normalize: False
mask_type: null #"improved"  
mask_threshold: null
normalize_type: "norm_by_voxel"
reject_outliers: null
limit_input_to: 10000 #null #500000 #594000

# Noise 
noisevariance: [30, 80]

# Augmentations
affine: 
  apply: True
  rotate: [-15,15] #rotation in degrees
  shift: [-0.15,0.15] # shift as a fraction of width and height
  scale: [0.7, 1.3] # scale w.r.t to unit of 1 
  shear: [-10,10] # shear angle in degrees
  intensity: null
  random_flip_prob: null
  sampletype: "uniform"
  
# Resume Training
# resume_training: null
# resume_epochs: 0
# resume_epochs: 35
#resume_training: "/home/ch215616/w/mwf_data/synthetic_data/trained_weights/anima_large_scale/anima_popvar_mwf_20_std2mean_10_v2_CSF_nonlin_10000brainss_train_nonlimited_smaller/ckpt-72-4.24.h5"
#resume_epochs: 73
resume_training: "latest"

# Output 
custom_checkpoint_dir: null

# Transfer Learning 
transfer_learning: False
pretrained_weights: null
trainable_layers: null

# Architecture 
network: "unet"
task: "reg" # [seg,class,reg]
patchsize: null
GAN: False
GANlossweight: null
regularize: "dropout" # "batchnorm"

# Unet parameters 
kernel_size: 3 
unet_resolution_levels: 4 #5 #4 # note - 7 levels return OOM and 6 levels have shape mismatch
conv_times: 3 #2 #3
filters_init: 64 #128 #64
dropout_level: 0.5 #0.1

# Callback Options
save_weights_only: True
save_best_only: False
save_frequency: 1
early_stop_patience: 2000

# Legacy 
ismrm: False
labels_source: null # ['julia','julia_norm','miml']
training_data_type: null # ['synthetic', 'real']

# Test 
trained_weights: null
epoch: "latest"
savedir: null
test_dir: null
test_regexp: null
test_file: null
verbose: "off"

# Hyperparameter Tuner
kerastuner: False
maxepochs: 8
executions_per_trial: 2                                                                      # make sure that seed is not set to fixed for this
tunerconfig: 
  # Key training features
  optimizer: 
    fix: False
    default: "adam"
    values: ["adam", "sgd"]
  learning_rate_schedule: 
    fix: True
    default: 0.00008
    values: [1e-3,8e-6]
    sampling: "LOG"
  learning_rate: 
    fix: False
    default: 0.00008
    values: [1e-3,8e-6]
    sampling: "LOG"
  batchsize: 
    fix: True
    default: 2
    values: [2,8,16,32]
  loss_function: 
    fix: True
    default: "ivim_loss"    
    values: ["mean_squared_error","mean_absolute_error", "mean_squared_logarithmic_error"]   # could also test combined losses - e.g. supervised + self-supervised + gan
  limit_input_to: 
    fix: True
    default: 100
    values_percent: [20,40,60,80,100]    
  random_seed: 
    fix: True
    default: 1
    values: [1,5,10,30,100]        
  # Regularization
  dropout_level: 
    fix: False
    default: 0.1
    values: [0.1, 0.3]
    step: 0.05
  # UNET design features    
  kernel_size: 
    fix: False
    default: 3
    values: [3,5,7]
  unet_resolution_levels: 
    fix: False
    default: 4
    values: [2,3,4]
  conv_times: 
    fix: False
    default: 3
    values: [2,3,4]
  filters_init: 
    fix: False
    default: 64
    values: [32,64]    
  pooling: 
    fix: False
    default: max
    values: ["average","max"]        
  # Activation Functions
  activation: 
    fix: True
    default: "relu"
    values: ["relu", "tanh", "sigmoid"]



